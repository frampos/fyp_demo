{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "t_shirt_detection_demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpYwRoSUui_2",
        "colab_type": "text"
      },
      "source": [
        "#T-Shirt Detection Attempt\n",
        "\n",
        "I train a custom object tetector with T-shirts for my final year project. Credits to [Tony607's object detection demo](https://github.com/Tony607/object_detection_demo) which formed the backbone of the implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhzxsJb3dpWq",
        "colab_type": "text"
      },
      "source": [
        "##Setting Up Chosen Model and Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnNXNQCjdniL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#training steps\n",
        "train_steps = 1000\n",
        "\n",
        "#validation steps\n",
        "test_steps = 50\n",
        "\n",
        "#name of the object detection model to use\n",
        "MODEL = 'ssd_mobilenet_v2_coco_2018_03_29'\n",
        "\n",
        "#name of the pipeline file in tensorflow object detection API\n",
        "model_config = 'ssd_mobilenet_v2_coco.config'\n",
        "\n",
        "#how much the model will feed through the network every increment\n",
        "batch_size = 12"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4V-XE6kbkc1",
        "colab_type": "text"
      },
      "source": [
        "## Clone My Repo to the Colab  Folder\n",
        "\n",
        "We do this so that we can work with the files needed for training and testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxc3DmvLQF3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "#my FYP repo\n",
        "my_repo = 'https://github.com/frampos/fyp_demo'\n",
        "\n",
        "#'%' executes certain magic commands (ipython)\n",
        "#change directory to content folder\n",
        "#/content is the root folder of Google Colab and has to be appended to all paths used in the notebook\n",
        "%cd /content\n",
        "\n",
        "#refer to path of the cloned repo regardless of the location of the repo\n",
        "cloned_dir = os.path.abspath(os.path.join('.', os.path.basename(my_repo)))\n",
        "\n",
        "#take changes from remote repo into the current\n",
        "!git clone {my_repo}\n",
        "%cd {cloned_dir}\n",
        "!git pull\n",
        "\n",
        "print(\"Cell Done.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9y4nCIGTm4S",
        "colab_type": "text"
      },
      "source": [
        "##TensorFlow Object Detection API and its dependencies\n",
        "\n",
        "I follow the steps for installing the API [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md). The code below installs the API and its dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecpHEnka8Kix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content\n",
        "\n",
        "#clone the TensorFlow API itself\n",
        "!git clone https://github.com/tensorflow/models.git\n",
        "\n",
        "#\"protobuf compiler\" installation\n",
        "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "\n",
        "#remaining libraries\n",
        "!pip install Cython\n",
        "!pip install contextlib2\n",
        "!pip install jupyter\n",
        "!pip install matplotlib\n",
        "!pip install pillow\n",
        "!pip install pycocotools\n",
        "!pip install numpy==1.17.5\n",
        "\n",
        "%cd /content/models/research\n",
        "\n",
        "#execute protobuf compilation\n",
        "#the compiler converts .proto files to .py files\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "#add Libraries to PYTHONPATH\n",
        "#maintain directories of custom Python libraries\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'\n",
        "\n",
        "#verifying installation\n",
        "!python object_detection/builders/model_builder_test.py\n",
        "\n",
        "print(\"Cell Done.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-k7uGThXlny",
        "colab_type": "text"
      },
      "source": [
        "##Generate TFRecords\n",
        "\n",
        "API requires TFRecords for training and testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezGDABRXXhPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd {cloned_dir}\n",
        "\n",
        "#pass arguments to xml_to_csv.py to convert the .xml annotations to one .csv file\n",
        "!python xml_to_csv.py\n",
        "!python generate_tfrecord.py --csv_input=data/annotations/train_labels.csv  --image_dir=data/train --output_path=data/annotations/train.record\n",
        "!python generate_tfrecord.py --csv_input=data/annotations/test_labels.csv  --image_dir=data/test --output_path=data/annotations/test.record\n",
        "\n",
        "#assign the records and label map to variables\n",
        "train_record = '/content/fyp_demo/data/annotations/train.record'\n",
        "test_record = '/content/fyp_demo/data/annotations/test.record'\n",
        "label_map_pbtxt = '/content/fyp_demo/data/annotations/label_map.pbtxt'\n",
        "\n",
        "print(\"Cell Done.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCNYAaC7w6N8",
        "colab_type": "text"
      },
      "source": [
        "## Download Base Model\n",
        "\n",
        "The base model comes from the [TensorFlow model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) and comes as a compressed .tar file.\n",
        "\n",
        "*Disclaimer: This cell was not written by me but was used as part of the implementation. Code was taken from [here](https://gist.github.com/AlaaSenjab/ac61b3b23793f9e07c2aa6ca0cff3285).*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orDCj6ihgUMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/models/research\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import urllib.request\n",
        "import tarfile\n",
        "\n",
        "#base model comes as a tar file, this concatenates the name defined earlier and appends the tar extension to the string\n",
        "MODEL_FILE = MODEL + '.tar.gz'\n",
        "\n",
        "#link to base model downloaded\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "\n",
        "#save destination\n",
        "DEST_DIR = '/content/models/research/pretrained_model'\n",
        "\n",
        "if not (os.path.exists(MODEL_FILE)):\n",
        "    #model name is downloaded \n",
        "    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "\n",
        "#must extract the tarfile\n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "#removes the unextracted tar file once the extraction is complete\n",
        "os.remove(MODEL_FILE)\n",
        "if (os.path.exists(DEST_DIR)):\n",
        "    shutil.rmtree(DEST_DIR)\n",
        "os.rename(MODEL, DEST_DIR)\n",
        "\n",
        "print(\"Cell Done.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvwtHlLOeRJD",
        "colab_type": "text"
      },
      "source": [
        "## Changing the Model Configuration\n",
        "\n",
        "The model config file contains the information of the neural network layers and feature extraction behaviour. There a many configs already available within the TensorFlow object detection API so I decided to use one.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIhw7IdpLuiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "#full path of the model configuration\n",
        "config_path = os.path.join('/content/models/research/object_detection/samples/configs/', model_config)\n",
        "assert os.path.isfile(config_path), '`{}` not exist'.format(config_path)\n",
        "\n",
        "print(\"Cell Done.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se97alPynXfB",
        "colab_type": "text"
      },
      "source": [
        "*Disclaimer: The cell below was not written by me but was used as part of the implementation. Code was taken from [here](https://gist.github.com/Said-Akbar/2d749d497ec6c928037660b6cab3bcb3).*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjtCbLF2i0wI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#regrex library\n",
        "import re\n",
        "\n",
        "#set the checkpoint destination folder\n",
        "fine_tune_checkpoint = os.path.join(DEST_DIR, \"model.ckpt\")\n",
        "\n",
        "# the label map generated earlier would count only 1 class as there was only one class label in the .xml, \"T-shirt\"\n",
        "num_classes = 1\n",
        "\n",
        "#read in the .config file\n",
        "with open(config_path) as f:\n",
        "    s = f.read()\n",
        "with open(config_path, 'w') as f:\n",
        "    \n",
        "    # fine_tune_checkpoint\n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
        "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
        "    \n",
        "    # tfrecord files train and test\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(train.record)(.*?\")', 'input_path: \"{}\"'.format(train_record), s)\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(val.record)(.*?\")', 'input_path: \"{}\"'.format(test_record), s)\n",
        "\n",
        "    # label_map_path\n",
        "    s = re.sub(\n",
        "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt), s)\n",
        "\n",
        "    # Set training batch_size.\n",
        "    s = re.sub('batch_size: [0-9]+',\n",
        "               'batch_size: {}'.format(batch_size), s)\n",
        "\n",
        "    # Set training steps, train_steps\n",
        "    s = re.sub('train_steps: [0-9]+',\n",
        "               'train_steps: {}'.format(train_steps), s)\n",
        "    \n",
        "    # Set number of classes num_classes\n",
        "    s = re.sub('num_classes: [0-9]+',\n",
        "               'num_classes: {}'.format(num_classes), s)\n",
        "    f.write(s)\n",
        "\n",
        "print(\"Cell Done.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8vULBwlPqPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dir = 'training/'\n",
        "\n",
        "# removes any old checkpoints when ran again\n",
        "!rm -rf {model_dir}\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "print(\"Cell Done.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yezX7FSLpOq",
        "colab_type": "text"
      },
      "source": [
        "##Connecting to TensorBoard\n",
        "\n",
        "Followed a [tutorial](https://www.dlology.com/blog/quick-guide-to-run-tensorboard-in-google-colab/) on how to connect to TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0H2PZs-mSCmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#use ngrok to tunnel the connection to your local machine\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "#extract the downloaded zip\n",
        "!unzip -o ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8o6r1o5SC5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#run TensorBoard in the background\n",
        "LOG_DIR = model_dir\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "print(\"Cell Done.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge1OX7gcSC7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#run ngrok to tunnel TensorBoard port 6006 to the outside world\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "print(\"Cell Done.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjhPT9iPSJ6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get public URL to access the colab TensorBoard\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDddx2rPfex9",
        "colab_type": "text"
      },
      "source": [
        "## Training Phase\n",
        "\n",
        "The API has defined a specific argument to pass to it in order to train the model. The .py file for this can be seen in `/content/models/research/object_detection/model_main.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjDHjhKQofT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pass arguemnet to model_man.py to train\n",
        "!python /content/models/research/object_detection/model_main.py \\\n",
        "    --logtostderr \\\n",
        "    --pipeline_config_path={config_path} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --num_train_steps={train_steps} \\\n",
        "    --num_eval_steps={test_steps} \\\n",
        "    \n",
        "print(\"Cell Done.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmSESMetj1sa",
        "colab_type": "text"
      },
      "source": [
        "## Export Post-Training Results\n",
        "Save the frozen inference graph for the object detection in the upcoming stages.\n",
        "\n",
        "Disclaimer: Code in the cell below was not written by me but was used as part of the system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "af9jSOEg14yD",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "#directory to save the frozen graph\n",
        "output_dir = './post_training_model'\n",
        "\n",
        "#finds the latest model checkpoint using regrex\n",
        "lst = os.listdir(model_dir)\n",
        "lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n",
        "steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n",
        "last_model = lst[steps.argmax()].replace('.meta', '')\n",
        "\n",
        "last_model_path = os.path.join(model_dir, last_model)\n",
        "\n",
        "#run export_inference_graph.py in the API using argument\n",
        "!python /content/models/research/object_detection/export_inference_graph.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path={config_path} \\\n",
        "    --output_directory={output_dir} \\\n",
        "    --trained_checkpoint_prefix={last_model_path}\n",
        "    \n",
        "print(\"Cell Done.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p09AOThWkaQv",
        "colab_type": "text"
      },
      "source": [
        "## Download the Frozen Inference File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ag9wsJEf2FNq",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "#full path for the inference graph\n",
        "frozen_inference = os.path.join(os.path.abspath(output_dir), \"frozen_inference_graph.pb\")\n",
        "assert os.path.isfile(frozen_inference), '`{}` not exist'.format(frozen_inference)\n",
        "\n",
        "print(\"Cell Done.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF5EiRsxbOf5",
        "colab_type": "text"
      },
      "source": [
        "#Object Detection and Visualisation\n",
        "\n",
        "Disclaimer: This is tutorial code within the API from `models/object_detection/object_detection_tutorial.ipynb` and was used as part of the implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxSAzU_vbkCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "#resize the images and put them in he new folder\n",
        "!python resize.py\n",
        "\n",
        "#path to the model checkpoint used for detection\n",
        "PATH_TO_CKPT = frozen_inference\n",
        "\n",
        "#path to labels\n",
        "PATH_TO_LABELS = label_map_pbtxt\n",
        "\n",
        "#path of the images for the inference test\n",
        "PATH_TO_TEST_IMAGES_DIR =  os.path.join(cloned_dir, \"test\")\n",
        "\n",
        "assert os.path.isfile(frozen_inference)\n",
        "assert os.path.isfile(PATH_TO_LABELS)\n",
        "TEST_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, \"*.*\"))\n",
        "\n",
        "print(\"Cell Done.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG5YUMdg1Po7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/models/research/object_detection\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "# This is needed to display the images.\n",
        "%matplotlib inline\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(\n",
        "    label_map, max_num_classes=num_classes, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "\n",
        "def load_image_into_numpy_array(image):\n",
        "    (im_width, im_height) = image.size\n",
        "    return np.array(image.getdata()).reshape(\n",
        "        (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "# Size, in inches, of the output images.\n",
        "IMAGE_SIZE = (12, 8)\n",
        "\n",
        "\n",
        "def run_inference_for_single_image(image, graph):\n",
        "    with graph.as_default():\n",
        "        with tf.Session() as sess:\n",
        "            # Get handles to input and output tensors\n",
        "            ops = tf.get_default_graph().get_operations()\n",
        "            all_tensor_names = {\n",
        "                output.name for op in ops for output in op.outputs}\n",
        "            tensor_dict = {}\n",
        "            for key in [\n",
        "                'num_detections', 'detection_boxes', 'detection_scores',\n",
        "                'detection_classes', 'detection_masks'\n",
        "            ]:\n",
        "                tensor_name = key + ':0'\n",
        "                if tensor_name in all_tensor_names:\n",
        "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "                        tensor_name)\n",
        "            if 'detection_masks' in tensor_dict:\n",
        "                # The following processing is only for single image\n",
        "                detection_boxes = tf.squeeze(\n",
        "                    tensor_dict['detection_boxes'], [0])\n",
        "                detection_masks = tf.squeeze(\n",
        "                    tensor_dict['detection_masks'], [0])\n",
        "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "                real_num_detection = tf.cast(\n",
        "                    tensor_dict['num_detections'][0], tf.int32)\n",
        "                detection_boxes = tf.slice(detection_boxes, [0, 0], [\n",
        "                                           real_num_detection, -1])\n",
        "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n",
        "                                           real_num_detection, -1, -1])\n",
        "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
        "                detection_masks_reframed = tf.cast(\n",
        "                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "                # Follow the convention by adding back the batch dimension\n",
        "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
        "                    detection_masks_reframed, 0)\n",
        "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "            # Run inference\n",
        "            output_dict = sess.run(tensor_dict,\n",
        "                                   feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "\n",
        "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "            output_dict['num_detections'] = int(\n",
        "                output_dict['num_detections'][0])\n",
        "            output_dict['detection_classes'] = output_dict[\n",
        "                'detection_classes'][0].astype(np.uint8)\n",
        "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "            if 'detection_masks' in output_dict:\n",
        "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "    return output_dict\n",
        "\n",
        "\n",
        "#Visualising the detection boxes and plotting the test images\n",
        "for image_path in TEST_IMAGE_PATHS:\n",
        "    image = Image.open(image_path)\n",
        "    # the array based representation of the image will be used later in order to prepare the\n",
        "    # result image with boxes and labels on it.\n",
        "    image_np = load_image_into_numpy_array(image)\n",
        "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "    # Actual detection.\n",
        "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "    # Visualization of the results of a detection.\n",
        "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np,\n",
        "        output_dict['detection_boxes'],\n",
        "        output_dict['detection_classes'],\n",
        "        output_dict['detection_scores'],\n",
        "        category_index,\n",
        "        instance_masks=output_dict.get('detection_masks'),\n",
        "        use_normalized_coordinates=True,\n",
        "        line_thickness=8)\n",
        "    plt.figure(figsize=IMAGE_SIZE)\n",
        "    plt.imshow(image_np)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}